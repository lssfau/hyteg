/*
 * Copyright (c) 2025 Marcus Mohr.
 *
 * This file is part of HyTeG
 * (see https://i10git.cs.fau.de/hyteg/hyteg).
 *
 * This program is free software: you can redistribute it and/or modify
 * it under the terms of the GNU General Public License as published by
 * the Free Software Foundation, either version 3 of the License, or
 * (at your option) any later version.
 *
 * This program is distributed in the hope that it will be useful,
 * but WITHOUT ANY WARRANTY; without even the implied warranty of
 * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 * GNU General Public License for more details.
 *
 * You should have received a copy of the GNU General Public License
 * along with this program. If not, see <http://www.gnu.org/licenses/>.
 */

#include "core/Environment.h"
#include "core/logging/Logging.h"

#include "hyteg/mesh/MeshInfo.hpp"
#include "hyteg/primitivestorage/SetupPrimitiveStorage.hpp"
#include "hyteg/primitivestorage/loadbalancing/SimpleBalancer.hpp"

/**
 * \page BH.04_Local_Primitives Tutorial BH.04 - Local Primitives and Neighborhood Information
 *
 * \dontinclude tutorials/basics-of-hyteg/BH.04_LocalPrimitives+Neighborhood.cp
 *
 * \brief In this tutorial we take a closer look at dealing with the local primitives of an MPI process.
 *
 * \section BH04-Intro Introduction
 *
 * One of the key features of HyTeG is that it splits the entities (triangles or tetrahedra) of the coarse
 * base mesh into their geometric components (vertices, edges, faces, and cells). This results in what is
 * referred to as <em>macro-primitives</em>. These play a key role as they act as containers for all the
 * data in our simulation.
 *
 * For parallel execution HyTeG distributes the macro-primitives between the different MPI processes.
 * While information on a certain primitive, such as e.g. the coordinates of a macro-vertex, will typically
 * be available on multiple processes, the data attached to a primitive are only directly available on
 * the process that <em>owns</em> the primitive. Such a primitive belongs to the set of <em>local
 * primitives</em> of that MPI process.
 *
 * If you are not, yet, familiar with the generation of a SetupPrimitiveStorage and a PrimitiveStorage, it
 * might be a good idea to first take a look at tutorial \ref BA.01_PrimitiveStorage.
 *
 * \section BH04-mesh Mesh generation and PrimitiveIDs
 *
 * We start by creating a simple mesh for the unit square that only consists of two triangles.
 *
 * \snippet{trimleft} this PrimitiveStorage
 *
 * Having a mesh with two triangles gives us 11 primitives altogether:
 * - 2 faces 
 * - 5 edges
 * - 4 vertices
 *
 * <img src="BH.04_PrimitivesUnitSquareMesh-Types.jpg" width="50%" />
 *
 * Each of these does have a unique PrimitiveID. In our setting without adaptive refinement
 * of the base mesh, this PrimitiveID is essentially a number obtained by enumerating the
 * primitives.
 *
 * Both, the SetupPrimitiveStorage class as well as the PrimitiveStorage class, provide various
 * methods to obtain information on the (local) primitives or pointers to the primitives themselves.
 * In our tutorial every process generates the complete mesh. We can, thus, access all primitives
 * from the resulting SetupPrimitiveStorage on every process. We do this below to determine the
 * PrimitiveIDs of our 11 primitives. Note that the output is independent of the number of processes
 * we use, as it is only generated by the "root" process with rank 0.
 *
 * \snippet{trimleft} this GetPrimitiveIDs
 *
 * Running our code we get an output similar to the following one
 *
 * \verbatim
[0][INFO    ]------(0.000 sec) Vertex 0 has coordinates (0, 0)
[0][INFO    ]------(0.000 sec) Vertex 1 has coordinates (1, 0)
[0][INFO    ]------(0.001 sec) Vertex 2 has coordinates (0, 1)
[0][INFO    ]------(0.001 sec) Vertex 3 has coordinates (1, 1)
[0][INFO    ]------(0.001 sec) Edge 4 connects vertices 0 <-> 1
[0][INFO    ]------(0.001 sec) Edge 5 connects vertices 0 <-> 2
[0][INFO    ]------(0.001 sec) Edge 6 connects vertices 0 <-> 3
[0][INFO    ]------(0.001 sec) Edge 7 connects vertices 1 <-> 3
[0][INFO    ]------(0.001 sec) Edge 8 connects vertices 2 <-> 3
[0][INFO    ]------(0.001 sec) Face 9 is formed by vertices 0 <-> 1 <-> 3
[0][INFO    ]------(0.001 sec) Face 10 is formed by vertices 0 <-> 2 <-> 3
\endverbatim
 *
 * <img src="BH.04_PrimitivesUnitSquareMesh.jpg" width="50%" />
 *
 * \section BH04-local Accessing the Local Primitives
 *
 * When we generate a PrimitiveStorage object from our SetupPrimitiveStorage and run in parallel the
 * primitives will be distributed amongst the MPI processes. The SetupPrimitiveStorage actually already
 * stores the information which primitive belongs to which process. We had generated this by calling
 * loadbalancing::roundRobin(). The constructor of the PrimitiveStorage on each process will simply
 * pick those primitives marked with its own MPI rank.
 *
 * In the following code we first query on each MPI process the number of local primitives from the
 * PrimitiveStorage and then for each type of primitive ask it for maps composed of the corresponding
 * PrimitiveID and a shared pointer to the Primitive itself. From the latter we obtain the PrimitiveID.
 * Note that this indirection is, of course, not necessary. We could directly use the ID from the map.
 * However, we want to demonstrate accessing the primitive through the shared pointer, which we often
 * need to do to obtain further information, such as coordinates, IDs of associated lower-dimensional
 * primitives (e.g. the faces of a cell) and the like.
 *
 * \snippet{trimleft} this LocalPrimitives
 *
 * Running our code with three MPI processes will give output such as the following
 *
 * \verbatim
0][INFO    ]------(0.001 sec) 
[0][INFO    ]------(0.001 sec) MPI rank 0 locally holds 4 primitives from the global set
[0][INFO    ]------(0.001 sec) MPI rank 0 holds VERTEX 0
[0][INFO    ]------(0.001 sec) MPI rank 0 holds VERTEX 3
[0][INFO    ]------(0.001 sec) MPI rank 0 holds EDGE 6
[0][INFO    ]------(0.001 sec) MPI rank 0 holds FACE 9
[0][INFO    ]------(0.001 sec) 
[1][INFO    ]------(0.001 sec) 
[1][INFO    ]------(0.001 sec) MPI rank 1 locally holds 4 primitives from the global set
[1][INFO    ]------(0.001 sec) MPI rank 1 holds VERTEX 1
[1][INFO    ]------(0.001 sec) MPI rank 1 holds EDGE 4
[1][INFO    ]------(0.001 sec) MPI rank 1 holds EDGE 7
[1][INFO    ]------(0.001 sec) MPI rank 1 holds FACE 10
[1][INFO    ]------(0.001 sec) 
[2][INFO    ]------(0.001 sec) 
[2][INFO    ]------(0.001 sec) MPI rank 2 locally holds 3 primitives from the global set
[2][INFO    ]------(0.001 sec) MPI rank 2 holds VERTEX 2
[2][INFO    ]------(0.001 sec) MPI rank 2 holds EDGE 5
[2][INFO    ]------(0.001 sec) MPI rank 2 holds EDGE 8
\endverbatim
 *
 * Visualising this information results in the following association where the primitives
 * local to rank 0 are marked in green, those of rank 1 are marked in red and those of
 * rank 2 in yellow:
 *
 * <img src="BH.04_PrimitivesUnitSquareMesh-Ranks.jpg" width="50%" />
 *
 * Putting this into the form of a table, we easily recognise the round-robin distribution
 *
 * <table>
 * <tr>
 * <td>PrimitiveID</td>
 * <td> 0</td>
 * <td> 1</td>
 * <td> 2</td>
 * <td> 3</td>
 * <td> 4</td>
 * <td> 5</td>
 * <td> 6</td>
 * <td> 7</td>
 * <td> 8</td>
 * <td> 9</td>
 * <td>10</td>
 * </tr>
 * <tr>
 * <td>MPI rank</td>
 * <td> 0</td>
 * <td> 1</td>
 * <td> 2</td>
 * <td> 0</td>
 * <td> 1</td>
 * <td> 2</td>
 * <td> 0</td>
 * <td> 1</td>
 * <td> 2</td>
 * <td> 0</td>
 * <td> 1</td>
 * </tr>
 * </table>
 *
 * \section BH04-nbrs Information on Neighboring Primitives
 *
 * In addition to the local primitives an MPI process also stores information on those
 * primitives that are neighbors of its own primitives. By default, this is restricted
 * to the direct neighbors. However, if necessary, e.g. in simulations using Discontinous
 * Galerkin elements, this can be extended by setting the parameter additionalHaloDepth
 * to a value > 0 when constructing the PrimitiveStorage.
 *
 * One important aspect where this neighborhood information is required is for the
 * exchange of simulation data via MPI communication.
 *
 * In the following snippet we query from our PrimitiveStorage the ids of the neighboring
 * primitives and use these to determine the neighbor's type and which other MPI process
 * owns it:
 *
 * \snippet{trimleft} this NeighborInfo
 * 
 * Running again with three MPI processes gives the following output
 * \verbatim
[0][INFO    ]------(0.001 sec) MPI rank 0 holds info on neighbor primitive 1 which is a VERTEX on process #1
[0][INFO    ]------(0.001 sec) MPI rank 0 holds info on neighbor primitive 4 which is a EDGE on process #1
[0][INFO    ]------(0.001 sec) MPI rank 0 holds info on neighbor primitive 5 which is a EDGE on process #2
[0][INFO    ]------(0.001 sec) MPI rank 0 holds info on neighbor primitive 7 which is a EDGE on process #1
[0][INFO    ]------(0.001 sec) MPI rank 0 holds info on neighbor primitive 8 which is a EDGE on process #2
[0][INFO    ]------(0.001 sec) MPI rank 0 holds info on neighbor primitive 10 which is a FACE on process #1
[0][INFO    ]------(0.001 sec) 
[1][INFO    ]------(0.001 sec) MPI rank 1 holds info on neighbor primitive 0 which is a VERTEX on process #0
[1][INFO    ]------(0.001 sec) MPI rank 1 holds info on neighbor primitive 2 which is a VERTEX on process #2
[1][INFO    ]------(0.001 sec) MPI rank 1 holds info on neighbor primitive 3 which is a VERTEX on process #0
[1][INFO    ]------(0.001 sec) MPI rank 1 holds info on neighbor primitive 5 which is a EDGE on process #2
[1][INFO    ]------(0.001 sec) MPI rank 1 holds info on neighbor primitive 6 which is a EDGE on process #0
[1][INFO    ]------(0.001 sec) MPI rank 1 holds info on neighbor primitive 8 which is a EDGE on process #2
[1][INFO    ]------(0.001 sec) MPI rank 1 holds info on neighbor primitive 9 which is a FACE on process #0
[1][INFO    ]------(0.001 sec) 
[2][INFO    ]------(0.002 sec) MPI rank 2 holds info on neighbor primitive 0 which is a VERTEX on process #0
[2][INFO    ]------(0.002 sec) MPI rank 2 holds info on neighbor primitive 3 which is a VERTEX on process #0
[2][INFO    ]------(0.002 sec) MPI rank 2 holds info on neighbor primitive 10 which is a FACE on process #1
\endverbatim
 *
 * We see that e.g. the process with rank 2 knows that the two vertices 0 and 3 and the face 10 are its
 * direct neighbors:
 *
 * <img src="BH.04_PrimitivesUnitSquareMesh-Nbrs-of-Rank2.jpg" width="50%" />
 *
 * \section BH04-Code Complete Program
 * \include tutorials/basics-of-hyteg/BH.04_LocalPrimitives+Neighborhood.cpp
 */

using namespace hyteg;

const std::map< Primitive::PrimitiveTypeEnum, std::string > primitiveTypeNames = {
    { Primitive::PrimitiveTypeEnum::VERTEX, "VERTEX" },
    { Primitive::PrimitiveTypeEnum::EDGE, "EDGE" },
    { Primitive::PrimitiveTypeEnum::FACE, "FACE" },
    { Primitive::PrimitiveTypeEnum::CELL, "CELL" },
    { Primitive::PrimitiveTypeEnum::INVALID, "INVALID" } };

void reportPrimitiveIDs( const SetupPrimitiveStorage& );
void reportLocalPrimitives( const std::shared_ptr< PrimitiveStorage >& storage );
void reportOnNeighbors( const std::shared_ptr< PrimitiveStorage >& storage );

// ======
//  main
// ======
int main( int argc, char* argv[] )
{
   // Setup enviroment
   walberla::Environment walberlaEnv( argc, argv );
   walberla::logging::Logging::instance()->setLogLevel( walberla::logging::Logging::INFO );
   walberla::MPIManager::instance()->useWorldComm();

   // Generate a simple mesh and PrimitiveStorage for the unit square
   /// [PrimitiveStorage]
   MeshInfo              meshInfo = MeshInfo::meshRectangle( Point2D( 0.0, 0.0 ), Point2D( 1.0, 1.0 ), MeshInfo::CROSS, 1, 1 );
   SetupPrimitiveStorage setupStorage( meshInfo, uint_c( walberla::mpi::MPIManager::instance()->numProcesses() ) );
   loadbalancing::roundRobin( setupStorage );
   std::shared_ptr< PrimitiveStorage > storage = std::make_shared< PrimitiveStorage >( setupStorage, 0 );
   /// [PrimitiveStorage]

   // Figure out which primitive has which ID
   reportPrimitiveIDs( setupStorage );
   /// [PrimitiveIDs]

   /// [PrimitiveIDs]

   // We serialize her on purpose to get better readable output (don't do this for production runs)
   for ( int procNum = 0; procNum < walberla::mpi::MPIManager::instance()->numProcesses(); ++procNum )
   {
      if ( walberla::mpi::MPIManager::instance()->rank() == procNum )
      {
         reportLocalPrimitives( storage );
      }
      WALBERLA_MPI_BARRIER();
   }

   for ( int procNum = 0; procNum < walberla::mpi::MPIManager::instance()->numProcesses(); ++procNum )
   {
      if ( walberla::mpi::MPIManager::instance()->rank() == procNum )
      {
         reportOnNeighbors( storage );
      }
      WALBERLA_MPI_BARRIER();
   }

   return EXIT_SUCCESS;
}

// ====================
//  reportPrimitiveIDs
// ====================
void reportPrimitiveIDs( const SetupPrimitiveStorage& setupStorage )
{
   /// [GetPrimitiveIDs]
   // print IDs as numbers w.r.t. the primitives of the (unrefined) base mesh
   PrimitiveIDFormatter::set( PrimitiveIDFormat::COARSE_ID );

   // access vertices
   std::map< PrimitiveID, std::shared_ptr< Vertex > > vertices = setupStorage.getVertices();
   for ( const auto& item : vertices )
   {
      Point3D coords = item.second->getCoordinates();
      WALBERLA_LOG_INFO_ON_ROOT( "Vertex " << item.first << " has coordinates (" << coords[0] << ", " << coords[1] << ")" );
   }

   // access edges
   std::map< PrimitiveID, std::shared_ptr< Edge > > edges = setupStorage.getEdges();
   for ( const auto& item : edges )
   {
      WALBERLA_LOG_INFO_ON_ROOT( "Edge " << item.first << " connects vertices " << item.second->getVertexID0() << " <-> "
                                         << item.second->getVertexID1() );
   }

   // access faces
   std::map< PrimitiveID, std::shared_ptr< Face > > faces = setupStorage.getFaces();
   for ( const auto& item : faces )
   {
      WALBERLA_LOG_INFO_ON_ROOT( "Face " << item.first << " is formed by vertices " << item.second->getVertexID0() << " <-> "
                                         << item.second->getVertexID1() << " <-> " << item.second->getVertexID2() );
   }
   /// [GetPrimitiveIDs]
}

// =======================
//  reportLocalPrimitives
// =======================
void reportLocalPrimitives( const std::shared_ptr< PrimitiveStorage >& storage )
{
   WALBERLA_LOG_INFO( "" );

   /// [LocalPrimitives]
   WALBERLA_LOG_INFO( "MPI rank " << walberla::mpi::MPIManager::instance()->rank() << " locally holds "
                                  << storage->getNumberOfLocalPrimitives() << " primitives from the global set" );

   // figure out this process' MPI rank
   int myRank = walberla::mpi::MPIManager::instance()->rank();

   // print IDs of local vertices
   for ( const auto& item : storage->getVertices() )
   {
      WALBERLA_LOG_INFO( "MPI rank " << myRank << " holds VERTEX " << item.second->getID() );
   }

   // print IDs of local edges
   for ( const auto& item : storage->getEdges() )
   {
      WALBERLA_LOG_INFO( "MPI rank " << myRank << " holds EDGE " << item.second->getID() );
   }

   // print IDs of local faces
   for ( const auto& item : storage->getFaces() )
   {
      WALBERLA_LOG_INFO( "MPI rank " << myRank << " holds FACE " << item.second->getID() );
   }
   /// [LocalPrimitives]

   WALBERLA_LOG_INFO( "" );
}

// ===================
//  reportOnNeighbors
// ===================
void reportOnNeighbors( const std::shared_ptr< PrimitiveStorage >& storage )
{
   int myRank = walberla::mpi::MPIManager::instance()->rank();

   /// [NeighborInfo]
   std::vector< PrimitiveID > nbrIDs;
   storage->getNeighboringPrimitiveIDs( nbrIDs );

   for ( const PrimitiveID& id : nbrIDs )
   {
      WALBERLA_ASSERT( storage->getPrimitiveType( id ) != Primitive::PrimitiveTypeEnum::INVALID );
      WALBERLA_LOG_INFO( "MPI rank " << myRank << " holds info on neighbor primitive " << id << " which is a "
                                     << primitiveTypeNames.at( storage->getPrimitiveType( id ) ) << " on process #"
                                     << storage->getPrimitiveRank( id ) );
   }
   /// [NeighborInfo]

   WALBERLA_LOG_INFO( "" );
}
